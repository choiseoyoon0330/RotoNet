# RotoNet: Rotoscoping-Based Artistic Style Transfer Networks
> **Abstract:** Conventional video style transfer techniques apply styles uniformly across entire frames, making it challenging to selectively transform specific objects. In this study, I propose RotoNet, a novel deep-learning framework that enables object-specific style transfer based on rotoscoping. RotoNet consists of an object tracking network and a style transfer network, aiming to selectively apply artistic styles to targeted objects within a video. By overcoming the limitations of existing style transfer models, RotoNet captures the distinctive aesthetic qualities of rotoscoping animation-precision in motion tracing, line expressiveness, and artistic interpretation of human movement.

## 1. Introduction


## 2. Methods
<img src="https://github.com/user-attachments/assets/aa24dbf5-2f3e-4eda-9112-bf4b27f02ebd"/>

The overall architecture of RotoNet consists of two main components designed to accurately track specific objects in a video and selectively apply style transformations. The object tracking network identifies the target object specified by the user in the initial frame and consistently segments and tracks the object throughout the entire sequence of video frames. The style transfer network utilizes the binary masks generated by the object tracking network to selectively apply style only to the designated object regions within the video.

### 2.1. Object Segmentation & Tracking  
For accurate object segmentation and tracking in videos, I employ [SAMURAI](https://github.com/yangchris11/samurai). SAMURAI introduces motion-based modeling and a motion-aware memory selection mechanism, enabling robust object tracking even in cluttered and dynamic environments. It supports **zero-shot** video object segmentation, allowing the target object to be segmented and tracked throughout the entire video using only a simple prompt—such as a box or mask—in the first frame, without any additional training. Built upon the Segment Anything Model (SAM), SAMURAI ensures strong segmentation performance and provides spatiotemporal consistency tailored for the video domain.

### 2.2. Video Stylization
Directly applying image style transfer models to video often leads to a lack of temporal consistency, resulting in frame-to-frame variations known as the "popping effect." To mitigate this temporal discontinuity, I adopt a blending strategy that combines the current frame with the previously stylized frame at a fixed ratio. This creates a **ghosting effect** that enhances temporal coherence across frames, ensuring more consistent stylization and reducing visual artifacts throughout the video.

## 3. Experiments
| Original            | Object Tracking & Segmentation              | 
|----------------------------|---------------------------|
|<img src="https://github.com/user-attachments/assets/22dcbc6d-9b56-4c08-a7fa-c1dae10c6e75" width="400"/>|<img src="https://github.com/user-attachments/assets/749f8815-a52a-4fb0-aefd-8b4d8a55bc4f" width="400"/>|

| Binary Mask            | Stylization             | 
|----------------------------|---------------------------|
|<img src="https://github.com/user-attachments/assets/4594b6d9-c3c0-4247-b9f8-f4c6251e6500" width="400"/>|<img src="https://github.com/user-attachments/assets/7773cfcc-f019-44fa-b91d-6c00edd47473" width="400"/>|


## 4. Result
### Style Image
<img src="https://github.com/user-attachments/assets/5e1c410e-93f9-4afe-b7d2-1e673b7f0755" width="400"/>

### Final Video
![final_video](https://github.com/user-attachments/assets/65797691-21fd-46c5-8cca-a91c861e11fb)


## Acknowledgement
This is the final project for the course &lt;Introduction to Generative AI>.
- [SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://github.com/yangchris11/samurai)
- [Neural Style Transfer Transition Video Processing](https://github.com/westgarthb/style-transfer-video-processor)
