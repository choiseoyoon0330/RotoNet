{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEMz08N7qFS3MNoqwKOJyf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## SAMURAI"],"metadata":{"id":"PiQlW-Xge_M9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSp8WJjdezur"},"outputs":[],"source":["import argparse\n","import os\n","import os.path as osp\n","import numpy as np\n","import cv2\n","import torch\n","import gc\n","import sys\n","sys.path.append(\"./sam2\")\n","from sam2.build_sam import build_sam2_video_predictor\n","\n","color = [(255, 0, 0)]\n","\n","def load_txt(gt_path):\n","    with open(gt_path, 'r') as f:\n","        gt = f.readlines()\n","    prompts = {}\n","    for fid, line in enumerate(gt):\n","        x, y, w, h = map(float, line.split(','))\n","        x, y, w, h = int(x), int(y), int(w), int(h)\n","        prompts[fid] = ((x, y, x + w, y + h), 0)\n","    return prompts\n","\n","def determine_model_cfg(model_path):\n","    if \"large\" in model_path:\n","        return \"configs/samurai/sam2.1_hiera_l.yaml\"\n","    elif \"base_plus\" in model_path:\n","        return \"configs/samurai/sam2.1_hiera_b+.yaml\"\n","    elif \"small\" in model_path:\n","        return \"configs/samurai/sam2.1_hiera_s.yaml\"\n","    elif \"tiny\" in model_path:\n","        return \"configs/samurai/sam2.1_hiera_t.yaml\"\n","    else:\n","        raise ValueError(\"Unknown model size in path!\")\n","\n","def prepare_frames_or_path(video_path):\n","    if video_path.endswith(\".mp4\") or osp.isdir(video_path):\n","        return video_path\n","    else:\n","        raise ValueError(\"Invalid video_path format. Should be .mp4 or a directory of jpg frames.\")\n","\n","def main(args):\n","    model_cfg = determine_model_cfg(args.model_path)\n","    predictor = build_sam2_video_predictor(model_cfg, args.model_path, device=\"cuda:0\")\n","    frames_or_path = prepare_frames_or_path(args.video_path)\n","    prompts = load_txt(args.txt_path)\n","\n","    frame_rate = 30\n","    if args.save_to_video:\n","        if osp.isdir(args.video_path):\n","            frames = sorted([osp.join(args.video_path, f) for f in os.listdir(args.video_path) if f.endswith((\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"))])\n","            loaded_frames = [cv2.imread(frame_path) for frame_path in frames]\n","            height, width = loaded_frames[0].shape[:2]\n","        else:\n","            cap = cv2.VideoCapture(args.video_path)\n","            frame_rate = cap.get(cv2.CAP_PROP_FPS)\n","            loaded_frames = []\n","            while True:\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break\n","                loaded_frames.append(frame)\n","            cap.release()\n","            height, width = loaded_frames[0].shape[:2]\n","\n","            if len(loaded_frames) == 0:\n","                raise ValueError(\"No frames were loaded from the video.\")\n","\n","    mask_dir = '/content/drive/MyDrive/RotoNet/samurai/masks'\n","    if not os.path.exists(mask_dir):\n","        os.makedirs(mask_dir)\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(args.video_output_path, fourcc, frame_rate, (width, height))\n","\n","    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float16):\n","        state = predictor.init_state(frames_or_path, offload_video_to_cpu=True)\n","        bbox, track_label = prompts[0]\n","        _, _, masks = predictor.add_new_points_or_box(state, box=bbox, frame_idx=0, obj_id=0)\n","\n","        for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n","            mask_to_vis = {}\n","            bbox_to_vis = {}\n","\n","            for obj_id, mask in zip(object_ids, masks):\n","                mask = mask[0].cpu().numpy()\n","                mask = mask > 0.0\n","                non_zero_indices = np.argwhere(mask)\n","                if len(non_zero_indices) == 0:\n","                    bbox = [0, 0, 0, 0]\n","                else:\n","                    y_min, x_min = non_zero_indices.min(axis=0).tolist()\n","                    y_max, x_max = non_zero_indices.max(axis=0).tolist()\n","                    bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n","                bbox_to_vis[obj_id] = bbox\n","                mask_to_vis[obj_id] = mask\n","\n","            if args.save_to_video:\n","                img = loaded_frames[frame_idx]\n","                for obj_id, mask in mask_to_vis.items():\n","                    mask_img = np.zeros((height, width, 3), np.uint8)\n","                    mask_img[mask] = color[(obj_id + 1) % len(color)]\n","                    img = cv2.addWeighted(img, 1, mask_img, 0.2, 0)\n","\n","                for obj_id, bbox in bbox_to_vis.items():\n","                    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), color[obj_id % len(color)], 2)\n","\n","                out.write(img)\n","\n","            for obj_id, mask in mask_to_vis.items():\n","                mask_filename = os.path.join(mask_dir, f\"frame{frame_idx:03d}_obj{obj_id}.png\")\n","                mask_img = np.zeros((height, width), dtype=np.uint8)\n","                mask_img[mask] = 255\n","                cv2.imwrite(mask_filename, mask_img)\n","\n","        if args.save_to_video:\n","            out.release()\n","\n","    del predictor, state\n","    gc.collect()\n","    torch.clear_autocast_cache()\n","    torch.cuda.empty_cache()\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--video_path\", required=True, help=\"Input video path or directory of frames.\")\n","    parser.add_argument(\"--txt_path\", required=True, help=\"Path to ground truth text file.\")\n","    parser.add_argument(\"--model_path\", default=\"sam2/checkpoints/sam2.1_hiera_large.pt\", help=\"Path to the model checkpoint.\")\n","    parser.add_argument(\"--video_output_path\", default=\"demo.mp4\", help=\"Path to save the output video.\")\n","    parser.add_argument(\"--save_to_video\", default=True, help=\"Save results to a video.\")\n","    args = parser.parse_args()\n","    main(args)"]},{"cell_type":"markdown","source":["## Style Transfer"],"metadata":{"id":"sjlX-92AfCkU"}},{"cell_type":"code","source":["class Config:\n","    ROOT_PATH = '.'\n","    # defines the maximum height dimension in pixels. Used for down-sampling the video frames\n","    FRAME_HEIGHT = 368\n","    CLEAR_INPUT_FRAME_CACHE = True\n","    # defines the rate at which you want to capture frames from the input video\n","    INPUT_FPS = 20\n","    INPUT_VIDEO_NAME = 'input_vid.mov'\n","    if uploaded_file:\n","      INPUT_VIDEO_NAME = list(uploaded_file.keys())[0]\n","    INPUT_VIDEO_PATH = f'{ROOT_PATH}/{INPUT_VIDEO_NAME}'\n","    INPUT_FRAME_DIRECTORY = f'{ROOT_PATH}/input_frames'\n","    INPUT_FRAME_FILE = '{:0>4d}_frame.png'\n","    INPUT_FRAME_PATH = f'{INPUT_FRAME_DIRECTORY}/{INPUT_FRAME_FILE}'\n","\n","    STYLE_REF_DIRECTORY = f'{ROOT_PATH}/style_ref'\n","    # defines the reference style image transition sequence. Values correspond to indices in STYLE_REF_DIRECTORY\n","    # add None in the sequence to NOT apply style transfer for part of the video (ie. [None, 0, 1, 2])\n","    STYLE_SEQUENCE = [0, 1, 2]\n","\n","    OUTPUT_FPS = 20\n","    OUTPUT_VIDEO_NAME = 'output_video.mp4'\n","    OUTPUT_VIDEO_PATH = f'{ROOT_PATH}/{OUTPUT_VIDEO_NAME}'\n","    OUTPUT_FRAME_DIRECTORY = f'{ROOT_PATH}/output_frames'\n","    OUTPUT_FRAME_FILE = '{:0>4d}_frame.png'\n","    OUTPUT_FRAME_PATH = f'{OUTPUT_FRAME_DIRECTORY}/{OUTPUT_FRAME_FILE}'\n","\n","    MASK_FRAME_DIRECTORY = f'{ROOT_PATH}/masks'\n","\n","    GHOST_FRAME_TRANSPARENCY = 0.1\n","    PRESERVE_COLORS = False\n","\n","    TENSORFLOW_CACHE_DIRECTORY = f'{ROOT_PATH}/tensorflow_cache'\n","    TENSORFLOW_HUB_HANDLE = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n"],"metadata":{"id":"QPPbOq8EfY89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Brycen Westgarth and Tristan Jogminas\n","# March 5, 2021\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow as tf\n","import glob\n","import cv2\n","import logging\n","from config import Config\n","\n","class StyleFrame:\n","\n","    MAX_CHANNEL_INTENSITY = 255.0\n","\n","    def __init__(self, conf=Config):\n","        self.conf = conf\n","        os.environ['TFHUB_CACHE_DIR'] = self.conf.TENSORFLOW_CACHE_DIRECTORY\n","        self.hub_module = hub.load(self.conf.TENSORFLOW_HUB_HANDLE)\n","        self.input_frame_directory = glob.glob(f'{self.conf.INPUT_FRAME_DIRECTORY}/*')\n","        self.output_frame_directory = glob.glob(f'{self.conf.OUTPUT_FRAME_DIRECTORY}/*')\n","        self.style_directory = glob.glob(f'{self.conf.STYLE_REF_DIRECTORY}/*')\n","        self.mask_frame_directory = sorted(glob.glob(f'{self.conf.MASK_FRAME_DIRECTORY}/*'))\n","        self.ref_count = len(self.conf.STYLE_SEQUENCE)\n","\n","        files_to_be_cleared = self.output_frame_directory\n","        if self.conf.CLEAR_INPUT_FRAME_CACHE:\n","            files_to_be_cleared += self.input_frame_directory\n","\n","        for file in files_to_be_cleared:\n","            os.remove(file)\n","\n","        # Update contents of directory after deletion\n","        self.input_frame_directory = glob.glob(f'{self.conf.INPUT_FRAME_DIRECTORY}/*')\n","        self.output_frame_directory = glob.glob(f'{self.conf.OUTPUT_FRAME_DIRECTORY}/*')\n","\n","        if len(self.input_frame_directory):\n","            # Retrieve an image in the input frame dir to get the width\n","            self.frame_width = cv2.imread(self.input_frame_directory[0]).shape[1]\n","\n","    def get_input_frames(self):\n","        if len(self.input_frame_directory):\n","            print(\"Using cached input frames\")\n","            return\n","        vid_obj = cv2.VideoCapture(self.conf.INPUT_VIDEO_PATH)\n","        total_frames = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT))\n","        fps = vid_obj.get(cv2.CAP_PROP_FPS)\n","\n","        success, image = vid_obj.read()\n","        if image is None:\n","            raise ValueError(f\"ERROR: Please provide missing video: {self.conf.INPUT_VIDEO_PATH}\")\n","        scale_constant = (self.conf.FRAME_HEIGHT / image.shape[0])\n","        self.frame_width = int(image.shape[1] * scale_constant)\n","\n","        count = 0\n","        while success:\n","            image = cv2.resize(image, (self.frame_width, self.conf.FRAME_HEIGHT))\n","            cv2.imwrite(self.conf.INPUT_FRAME_PATH.format(count), image.astype(np.uint8))\n","            count += 1\n","            success, image = vid_obj.read()\n","        self.input_frame_directory = glob.glob(f'{self.conf.INPUT_FRAME_DIRECTORY}/*')\n","\n","    def get_style_info(self):\n","        frame_length = len(self.input_frame_directory)\n","        style_refs = list()\n","        resized_ref = False\n","        style_files = sorted(self.style_directory)\n","        self.t_const = frame_length if self.ref_count == 1 else np.ceil(frame_length / (self.ref_count - 1))\n","\n","        # Open first style ref and force all other style refs to match size\n","        first_style_ref = cv2.imread(style_files.pop(0))\n","        first_style_ref = cv2.cvtColor(first_style_ref, cv2.COLOR_BGR2RGB)\n","        first_style_height, first_style_width, _rgb = first_style_ref.shape\n","        style_refs.append(first_style_ref / self.MAX_CHANNEL_INTENSITY)\n","\n","        for filename in style_files:\n","            style_ref = cv2.imread(filename)\n","            style_ref = cv2.cvtColor(style_ref, cv2.COLOR_BGR2RGB)\n","            style_ref_height, style_ref_width, _rgb = style_ref.shape\n","            # Resize all style_ref images to match first style_ref dimensions\n","            if style_ref_width != first_style_width or style_ref_height != first_style_height:\n","                resized_ref = True\n","                style_ref = cv2.resize(style_ref, (first_style_width, first_style_height))\n","            style_refs.append(style_ref / self.MAX_CHANNEL_INTENSITY)\n","\n","        if resized_ref:\n","            print(\"WARNING: Resizing style images which may cause distortion. To avoid this, please provide style images with the same dimensions\")\n","\n","        self.transition_style_seq = list()\n","        for i in range(self.ref_count):\n","            if self.conf.STYLE_SEQUENCE[i] is None:\n","                self.transition_style_seq.append(None)\n","            else:\n","                self.transition_style_seq.append(style_refs[self.conf.STYLE_SEQUENCE[i]])\n","\n","    def _trim_img(self, img):\n","        return img[:self.conf.FRAME_HEIGHT, :self.frame_width]\n","\n","    def get_output_frames(self):\n","        self.input_frame_directory = glob.glob(f'{self.conf.INPUT_FRAME_DIRECTORY}/*')\n","        ghost_frame = None\n","        for count, filename in enumerate(sorted(self.input_frame_directory)):\n","            if count % 10 == 0:\n","                print(f\"Output frame: {(count/len(self.input_frame_directory)):.0%}\")\n","            content_img = cv2.imread(filename)\n","            content_img = cv2.cvtColor(content_img, cv2.COLOR_BGR2RGB) / self.MAX_CHANNEL_INTENSITY\n","            curr_style_img_index = int(count / self.t_const)\n","            mix_ratio = 1 - ((count % self.t_const) / self.t_const)\n","            inv_mix_ratio = 1 - mix_ratio\n","\n","            prev_image = self.transition_style_seq[curr_style_img_index]\n","            next_image = self.transition_style_seq[curr_style_img_index + 1]\n","\n","            prev_is_content_img = False\n","            next_is_content_img = False\n","            if prev_image is None:\n","                prev_image = content_img\n","                prev_is_content_img = True\n","            if next_image is None:\n","                next_image = content_img\n","                next_is_content_img = True\n","            # If both, don't need to apply style transfer\n","            if prev_is_content_img and next_is_content_img:\n","                temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n","                cv2.imwrite(self.conf.OUTPUT_FRAME_PATH.format(count), temp_ghost_frame)\n","                continue\n","\n","            if count > 0:\n","                content_img = ((1 - self.conf.GHOST_FRAME_TRANSPARENCY) * content_img) + (self.conf.GHOST_FRAME_TRANSPARENCY * ghost_frame)\n","            content_img = tf.cast(tf.convert_to_tensor(content_img), tf.float32)\n","\n","            if prev_is_content_img:\n","                blended_img = next_image\n","            elif next_is_content_img:\n","                blended_img = prev_image\n","            else:\n","                prev_style = mix_ratio * prev_image\n","                next_style = inv_mix_ratio * next_image\n","                blended_img = prev_style + next_style\n","\n","            blended_img = tf.cast(tf.convert_to_tensor(blended_img), tf.float32)\n","            expanded_blended_img = tf.constant(tf.expand_dims(blended_img, axis=0))\n","            expanded_content_img = tf.constant(tf.expand_dims(content_img, axis=0))\n","            # Apply style transfer\n","            stylized_img = self.hub_module(expanded_content_img, expanded_blended_img).pop()\n","            stylized_img = tf.squeeze(stylized_img)\n","\n","\n","            # Re-blend\n","            if prev_is_content_img:\n","                prev_style = mix_ratio * content_img\n","                next_style = inv_mix_ratio * stylized_img\n","            if next_is_content_img:\n","                prev_style = mix_ratio * stylized_img\n","                next_style = inv_mix_ratio * content_img\n","            if prev_is_content_img or next_is_content_img:\n","                stylized_img = self._trim_img(prev_style) + self._trim_img(next_style)\n","\n","            if self.conf.PRESERVE_COLORS:\n","                stylized_img = self._color_correct_to_input(content_img, stylized_img)\n","\n","            mask_path = self.mask_frame_directory[count]\n","            if os.path.exists(mask_path):\n","              mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","              _, mask = cv2.threshold(mask, 127, 1, cv2.THRESH_BINARY)\n","\n","              if mask.shape != content_img.shape[:2]:\n","                mask = cv2.resize(mask, (content_img.shape[1], content_img.shape[0]))\n","\n","              mask = np.stack([mask]*3, axis=-1).astype(np.float32)\n","\n","              if stylized_img.shape != content_img.shape:\n","                stylized_img = tf.image.resize(stylized_img, (content_img.shape[0], content_img.shape[1]))\n","                stylized_img = stylized_img.numpy()\n","\n","              blended_img = (stylized_img * mask) + (content_img * (1 - mask))\n","\n","            else:\n","              print(f\"No Mask: {mask_path}\")\n","              blended_img = content_img\n","\n","            ghost_frame = np.asarray(self._trim_img(blended_img))\n","\n","            temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n","            cv2.imwrite(self.conf.OUTPUT_FRAME_PATH.format(count), temp_ghost_frame)\n","        self.output_frame_directory = glob.glob(f'{self.conf.OUTPUT_FRAME_DIRECTORY}/*')\n","\n","    def _color_correct_to_input(self, content, generated):\n","        # image manipulations for compatibility with opencv\n","        content = np.array((content * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n","        content = cv2.cvtColor(content, cv2.COLOR_BGR2YCR_CB)\n","        generated = np.array((generated * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n","        generated = cv2.cvtColor(generated, cv2.COLOR_BGR2YCR_CB)\n","        generated = self._trim_img(generated)\n","        # extract channels, merge intensity and color spaces\n","        color_corrected = np.zeros(generated.shape, dtype=np.float32)\n","        color_corrected[:, :, 0] = generated[:, :, 0]\n","        color_corrected[:, :, 1] = content[:, :, 1]\n","        color_corrected[:, :, 2] = content[:, :, 2]\n","        return cv2.cvtColor(color_corrected, cv2.COLOR_YCrCb2BGR) / self.MAX_CHANNEL_INTENSITY\n","\n","\n","    def create_video(self):\n","        self.output_frame_directory = glob.glob(f'{self.conf.OUTPUT_FRAME_DIRECTORY}/*')\n","        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","        video_writer = cv2.VideoWriter(self.conf.OUTPUT_VIDEO_PATH, fourcc, self.conf.OUTPUT_FPS, (self.frame_width, self.conf.FRAME_HEIGHT))\n","\n","        for count, filename in enumerate(sorted(self.output_frame_directory)):\n","            if count % 10 == 0:\n","                print(f\"Saving frame: {(count/len(self.output_frame_directory)):.0%}\")\n","            image = cv2.imread(filename)\n","            video_writer.write(image)\n","\n","        video_writer.release()\n","        print(f\"Style transfer complete! Output at {self.conf.OUTPUT_VIDEO_PATH}\")\n","\n","    def run(self):\n","        print(\"Getting input frames\")\n","        self.get_input_frames()\n","        print(\"Getting style info\")\n","        self.get_style_info()\n","        print(\"Getting output frames\")\n","        self.get_output_frames()\n","        print(\"Saving video\")\n","        self.create_video()\n","\n","if __name__ == \"__main__\":\n","    StyleFrame().run()"],"metadata":{"id":"-kKRQ3xZfEuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from style_frames import StyleFrame\n","StyleFrame(Config).run()"],"metadata":{"id":"R_nVKuNgfn4v"},"execution_count":null,"outputs":[]}]}
